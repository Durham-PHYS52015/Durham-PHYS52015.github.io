<!doctype html><html lang=en dir=ltr>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<meta name=description content="Introduction #  This course provides a brief introduction to parallel computing. It focuses on those paradigms that are prevalent in scientific computing in academia. There are other parallel programming models which we will mention in passing where appropriate.
Task I think I have arranged for you to have a Hamilton account. Follow the instructions in that guide to log in and check that you can access Hamilton.
If you can&rsquo;t access Hamilton, get in touch with me!">
<meta name=theme-color content="#FFFFFF"><meta property="og:title" content="Introduction and motivation">
<meta property="og:description" content="Introduction #  This course provides a brief introduction to parallel computing. It focuses on those paradigms that are prevalent in scientific computing in academia. There are other parallel programming models which we will mention in passing where appropriate.
Task I think I have arranged for you to have a Hamilton account. Follow the instructions in that guide to log in and check that you can access Hamilton.
If you can&rsquo;t access Hamilton, get in touch with me!">
<meta property="og:type" content="article">
<meta property="og:url" content="https://durham-phys52015.github.io/notes/introduction/"><meta property="article:section" content="notes">
<meta property="article:modified_time" content="2022-03-03T14:52:34+00:00">
<title>Introduction and motivation | PHYS52015 – Introduction to HPC</title>
<link rel=manifest href=/manifest.json>
<link rel=icon href=/favicon.png type=image/x-icon>
<link rel=stylesheet href=/book.min.b85762cbec3aa7dfc1fa14ce1d900668d26349753560425c37dac8f9e08407d6.css integrity="sha256-uFdiy+w6p9/B+hTOHZAGaNJjSXU1YEJcN9rI+eCEB9Y=">
</head>
<body dir=ltr>
<input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control>
<main class="container flex">
<aside class=book-menu>
<nav>
<div class=book-brand><img class=book-center src=/logo.svg alt=Logo><h2>
<a href=/>
PHYS52015 – Introduction to HPC
</a>
</h2>
</div>
<ul>
<li>
<span>Administrivia</span>
<ul>
<li>
<a href=/setup/remote/>Remote editing/development</a>
</li>
<li>
<a href=/setup/hamilton-quickstart/>Hamilton access & quickstart</a>
</li>
<li>
<a href=/setup/byod/>Local setup</a>
</li>
<li>
<a href=/setup/configuration/>ssh configuration</a>
</li>
<li>
<a href=/setup/unix/>Unix resources</a>
</li>
</ul>
</li>
<li>
<span>Exercises</span>
<ul>
<li>
<a href=/exercises/hello/>Parallel Hello World</a>
</li>
<li>
<a href=/exercises/openmp-loop/>OpenMP: parallel loops</a>
</li>
<li>
<a href=/exercises/openmp-stencil/>OpenMP: stencils</a>
</li>
<li>
<a href=/exercises/openmp-reduction/>OpenMP: synchronisation</a>
</li>
<li>
<a href=/exercises/mpi-ring/>MPI: messages round a ring</a>
</li>
<li>
<a href=/exercises/mpi-pi/>MPI: Calculating π</a>
</li>
<li>
<a href=/exercises/mpi-ping-pong/>MPI: ping-pong latency</a>
</li>
<li>
<a href=/exercises/mpi-collectives/>MPI: simple collectives</a>
</li>
<li>
<a href=/exercises/mpi-stencil/>MPI: domain decomposition and halo exchanges</a>
</li>
</ul>
</li>
<li>
<a href=/coursework/>Coursework: stencils and collectives</a>
</li>
<li>
<span>Notes</span>
<ul>
<li>
<a href=/notes/introduction/ class=active>Introduction and motivation</a>
</li>
<li>
<span>Theory & concepts</span>
<ul>
<li>
<a href=/notes/theory/scaling-laws/>Parallel scaling laws</a>
</li>
<li>
<a href=/notes/theory/hardware-parallelism/>Parallelism in hardware: an overview</a>
</li>
<li>
<a href=/notes/theory/concepts/>Parallel patterns</a>
</li>
</ul>
</li>
<li>
<a href=/notes/openmp/>OpenMP</a>
<ul>
<li>
<a href=/notes/openmp/intro/>What is OpenMP?</a>
</li>
<li>
<a href=/notes/openmp/loop-parallelism/>Loop parallelism</a>
</li>
<li>
<a href=/notes/openmp/collectives/>Collectives</a>
</li>
</ul>
</li>
<li>
<a href=/notes/mpi/>MPI</a>
<ul>
<li>
<a href=/notes/mpi/point-to-point/>Point-to-point messaging in MPI</a>
</li>
<li>
<a href=/notes/mpi/point-to-point-nb/>Non-blocking point-to-point messaging</a>
</li>
<li>
<a href=/notes/mpi/collectives/>Collectives</a>
</li>
<li>
<a href=/notes/mpi/advanced/>Advanced topics</a>
</li>
</ul>
</li>
</ul>
</li>
<li>
<a href=/resources/>Further resources</a>
</li>
<li>
<a href=/acknowledgements/>Acknowledgements</a>
</li>
<li>
<span>Past editions</span>
<ul>
<li>
<span>2020/21</span>
<ul>
<li>
<a href=/past-editions/2020-21/coursework/>Coursework: parallel dense linear algebra</a>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
<script>(function(){var a=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(b){localStorage.setItem("menu.scrollTop",a.scrollTop)}),a.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>
</aside>
<div class=book-page>
<header class=book-header>
<div class="flex align-center justify-between">
<label for=menu-control>
<img src=/svg/menu.svg class=book-icon alt=Menu>
</label>
<strong>Introduction and motivation</strong>
<label for=toc-control>
<img src=/svg/toc.svg class=book-icon alt="Table of Contents">
</label>
</div>
<aside class="hidden clearfix">
<nav id=TableOfContents>
<ul>
<li><a href=#programming-languages>Programming languages</a></li>
<li><a href=#why-parallel-computing>Why parallel computing?</a></li>
<li><a href=#moores-law><a href=https://en.wikipedia.org/wiki/Moore%27s_law>Moore&rsquo;s Law</a></a></li>
<li><a href=#consequences>Consequences</a></li>
<li><a href=#where-next>Where next?</a></li>
</ul>
</nav>
</aside>
</header>
<article class=markdown><h1 id=introduction>
Introduction
<a class=anchor href=#introduction>#</a>
</h1>
<p>This course provides a brief introduction to parallel computing. It
focuses on those paradigms that are prevalent in scientific computing
in academia. There are other parallel programming models which we will
mention in passing where appropriate.</p>
<blockquote class=task>
<h3>Task</h3>
<p>I think I have arranged for you to have a <a href=https://durham-phys52015.github.io/setup/hamilton-quickstart/>Hamilton account</a>. Follow the instructions in that guide
to log in and check that you can access Hamilton.</p>
<p>If you can&rsquo;t access Hamilton, get in touch with me!</p>
<p>If you have a COSMA account, you can use that instead, and therefore
need not use Hamilton.</p>
</blockquote>
<h2 id=programming-languages>
Programming languages
<a class=anchor href=#programming-languages>#</a>
</h2>
<p>This course requires that you know, and can program in,
<a href=https://en.wikipedia.org/wiki/C_%28programming_language%29>C</a>. If you&rsquo;re
used to high-level languages like Python, this can be a bit of a
struggle when first starting. For a very brief &ldquo;orientation&rdquo; to the
ways in which C differs from other languages, I recommend Simon
Tatham&rsquo;s <a href=https://www.chiark.greenend.org.uk/~sgtatham/cdescent/>descent to
C</a>. For a
textbook, I recommend
<a href=https://en.wikipedia.org/wiki/The_C_Programming_Language>K&R</a>, of
which there are a number of copies <a href="https://library.dur.ac.uk/record=b1370073~S1">in the
library</a>.</p>
<h2 id=why-parallel-computing>
Why parallel computing?
<a class=anchor href=#why-parallel-computing>#</a>
</h2>
<p>Computing in parallel is <em>more difficult</em> than computing in serial. We
often have to write more code, or use more complicated libraries. If
we are going to commit to doing it, there must be some advantage.</p>
<p>Our goal when running scientific simulations or data analyses is often
to increase the resolution of the simulation, run more
replicates (for example more bootstrapping samples in a statistical
test), obtain answers faster, or some combination thereof.</p>
<p>In some cases, we might be analysing or simulating such
high-resolution data that it does not fit in the memory of a single
computer. Or, the problem may take so long that running it one a
single computer would require us to wait a week or longer for the
result.</p>
<p>For example, the <a href=https://www.metoffice.gov.uk>UK Met Office</a>
produces a number of weather forecasts for the UK at various
resolutions multiple <a href=https://www.metoffice.gov.uk/research/approach/modelling-systems/unified-model/weather-forecasting>times a
day</a>.
This includes an hourly run making predictions for the next 12 hours.
At the very least, a forecast is useless if it is slower than real
time, and the target here is that every 12 hour forecast runs in under
one hour (more than 12x real time). The model over the UK is at a
<a href=https://www.metoffice.gov.uk/research/approach/modelling-systems/unified-model/weather-forecasting>resolution of
1.5km</a>.
This produces a computer problem that is large enough that each run is
parallelised across 500 compute cores (so that they can hit their
operational window of taking less than one hour to produce a
forecast). To do this, the data, and algorithmic work that produce the
model answers must be split up across these processes.</p>
<p>A perhaps simpler example comes when performing various forms of Monte
Carlo integration. Of which we will see a very simple example in <a href=https://durham-phys52015.github.io/exercises/mpi-pi/>one
of the exercises</a>. The accuracy of Monte
Carlo estimators increases when we add more samples. These samples are
often completely independent. If we want to generate more samples (to
improve accuracy), we can do so by running a single process code for
longer, or dividing up the work of generating samples among multiple
processes.</p>
<h2 id=moores-law>
<a href=https://en.wikipedia.org/wiki/Moore%27s_law>Moore&rsquo;s Law</a>
<a class=anchor href=#moores-law>#</a>
</h2>
<blockquote>
<p>The number of transistors in an integrated circuit doubles about
every two years.</p>
</blockquote>
<p>This was an observation Gordon Moore made about the <em>complexity</em> of
chips in the
<a href=https://newsroom.intel.com/wp-content/uploads/sites/11/2018/05/moores-law-electronics.pdf>mid-60s</a>.
That is, every 18-24 months, the computational
performance available at a fixed price doubles.</p>
<p>This trend can be observed when we look at the performance of systems
in the <a href=https://www.top500.org>Top 500</a>, which has been measuring the
floating point performance of the largest 500 supercomputers in the
world since 1993.</p>
<figure style=width:50%>
<img class=scaled src=https://durham-phys52015.github.io/images/auto/top500.svg alt="Performance trends of the Top500 list since inception."> <figcaption>
<p>Performance trends of the Top500 list since inception.</p>
</figcaption>
</figure>
<p>This seems great, however, there is a catch. If we zoom in and look
at a history of CPU trends we see a slightly different picture.</p>
<a href=https://github.com/karlrupp/microprocessor-trend-data>
<figure style=width:100%>
<img class=scaled src=https://durham-phys52015.github.io/images/manual/48-years-processor-trend.png alt="48 years of microprocessor trend data (image by Karl Rupp)"> <figcaption>
<p>48 years of microprocessor trend data (image by Karl Rupp)</p>
</figcaption>
</figure>
</a>
<p>Although the transistor counts still follow an exponential growth
curve, CPU clock speed stopped increasing in around 2004. As a
consequence, the <em>single thread</em> (SpecINT in the above figure)
performance of modern chips has almost flat-lined since 2008. This
Herb Sutter makes some nice observations about this in <a href=http://www.gotw.ca/publications/concurrency-ddj.htm><em>The Free
Lunch is Over</em></a>.</p>
<blockquote class=exercise>
<h3>Exercise</h3>
Read <a href=http://www.gotw.ca/publications/concurrency-ddj.htm><em>The Free Lunch is
Over</em></a>. Make a
note of what you think the key points are, and anything you didn&rsquo;t
understand, to bring to the live sessions.
</blockquote>
<h2 id=consequences>
Consequences
<a class=anchor href=#consequences>#</a>
</h2>
<p>If the number of transistors continues to increase, but the effective
single-thread performance of CPUs does not (or at least, does not at
the same rate), what are all these extra transistors being used for?</p>
<p>The answer is that they are used to provide parallelism. Notice how
since 2004, the number of logical cores on a CPU has risen from one to
typically around 32.</p>
<p>The consequence for us as programmers is that to use all the performance
that chips provide, we must write code that exposes and uses
parallelism.</p>
<h2 id=where-next>
Where next?
<a class=anchor href=#where-next>#</a>
</h2>
<p>We&rsquo;ll introduce the different types of parallelism offered by modern
supercomputers, and look at potential programming models and
parallelisation strategies.</p>
</article>
<footer class=book-footer>
<div class="flex flex-wrap justify-between">
<div><a class="flex align-center" href=https://github.com/Durham-PHYS52015/Durham-PHYS52015.github.io/commit/80e39a5a977842f89b17cfee95787f4886404c7e title="Last modified by Lawrence Mitchell | March 3, 2022" target=_blank rel=noopener>
<img src=/svg/calendar.svg class=book-icon alt=Calendar>
<span>March 3, 2022</span>
</a>
</div>
<div>
<a class="flex align-center" href=https://github.com/Durham-PHYS52015/Durham-PHYS52015.github.io/edit/main/site/content/notes/introduction.md target=_blank rel=noopener>
<img src=/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span>
</a>
</div>
</div>
<div class="flex flex-wrap align-right">
<p>
© 2020&ndash; <a href=mailto:lawrence.mitchell@durham.ac.uk>Lawrence Mitchell</a>, <a href=https://www.ippp.dur.ac.uk/~hschulz/>Holger Schulz</a>, <a href="https://www.dur.ac.uk/physics/staff/profiles/?mode=staff&id=16712">Christian Arnold</a> & <a href=https://www.dur.ac.uk/>Durham University</a>.
</p>
<p>
<a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/>
<img alt="Creative Commons License" style=border-width:0 src=/cc-by-sa.svg>
</a>
This work is licensed under a <a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/>Creative
Commons Attribution-ShareAlike 4.0 International License</a>.
</p>
</div>
</footer>
<label for=menu-control class="hidden book-menu-overlay"></label>
</div>
<aside class=book-toc>
<nav id=TableOfContents>
<ul>
<li><a href=#programming-languages>Programming languages</a></li>
<li><a href=#why-parallel-computing>Why parallel computing?</a></li>
<li><a href=#moores-law><a href=https://en.wikipedia.org/wiki/Moore%27s_law>Moore&rsquo;s Law</a></a></li>
<li><a href=#consequences>Consequences</a></li>
<li><a href=#where-next>Where next?</a></li>
</ul>
</nav>
</aside>
</main>
</body>
</html>